# Data Science Foundations — Monorepo

This repository is a curated **data science foundations monorepo**, covering the full progression from database setup to classical machine learning models.

It is structured by **real-world data roles**, reflecting how data projects evolve in practice: from data engineering and warehousing, through analysis, to modeling and evaluation.

The focus of this repository is on **code quality, data workflows, and reasoning**, rather than on proprietary datasets or pre-generated outputs.

⚠️ **Note**  
This repository reflects an evolving learning process and has been curated for public presentation.  
While the code is functional and structured, minor inconsistencies or edge cases may still exist.

---

## Repository Structure

```text
.
├── 00_data_engineer
├── 01_data_warehouse
├── 02_data_analyst
├── 03_data_scientist_01
├── 04_data_scientist_02
├── db_init
├── src
├── scripts
├── etl
├── Dockerfile
├── docker-compose.yml
├── Makefile
└── requirements.txt
Each module represents a distinct stage in a typical data lifecycle.

Modules Overview
00 — Data Engineer
Database setup and infrastructure foundations

Database initialization scripts

Table creation and schema design

Supporting utilities for automated setup

Focus: reproducible environments and reliable data foundations.

01 — Data Warehouse
SQL-based data preparation and consolidation

Structured data ingestion

Deduplication strategies

Dataset fusion into a single source of truth

Focus: transforming raw data into analysis-ready datasets using SQL.

02 — Data Analyst
Exploratory analysis and visualization logic

Aggregations and descriptive metrics

Visualization scripts (charts, distributions, clustering logic)

Analytical reasoning through code

Focus: understanding data behavior and extracting insights.

03 — Data Scientist I
Feature analysis and preprocessing

Feature comparison and correlation

Normalization and standardization

Dataset preparation for modeling

Focus: preparing clean, meaningful features for machine learning.

04 — Data Scientist II
Classical machine learning models and evaluation

Decision trees

k-nearest neighbors (KNN)

Feature selection techniques

Ensemble and voting strategies

Model evaluation logic

Focus: model behavior, interpretability, and evaluation pipelines.

Design Decisions
Datasets, subjects, and evaluation artifacts are intentionally excluded
This repository showcases data workflows, logic, and structure rather than academic or proprietary datasets.

Generated outputs (plots, predictions, truth files) are not versioned
Outputs can be reproduced by running the corresponding scripts when appropriate.

Clear separation of concerns
Each module focuses on a single stage of the data lifecycle, following a role-based progression.

Execution & Reproducibility
This repository does not include original datasets or generated outputs.

Most scripts are designed to be executed independently once the required data is available.
The emphasis is on reproducible logic and workflow design, not on reproducing exact numerical results.

When applicable:

SQL scripts can be executed against a PostgreSQL instance

Python scripts expect CSV inputs with documented or inferable schemas

Visualizations and predictions can be regenerated by running the scripts

Setup
Python environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
Docker (optional)
docker compose up --build
Purpose
This repository serves as:

a technical portfolio of data foundations

a reference for structured data workflows

a demonstration of progression from data engineering to data science

It reflects a practical, production-aware approach to working with data.

Background
This project was originally developed in an academic setting and later refactored and curated
to meet professional and public portfolio standards.



