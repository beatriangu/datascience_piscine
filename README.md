# Data Science Foundations â€” End-to-End Project

This repository is a **Data Science Foundations project** covering the full data lifecycle, from
database initialization and data warehousing to exploratory analysis and classical machine learning models.

The project is structured by **real-world data roles**, reflecting how data products are built in practice:
from data engineering and SQL pipelines to data analysis, feature engineering, and modeling.

> âš ï¸ **Disclaimer**  
> This repository reflects a learning and consolidation process.  
> The code is functional and structured for clarity and reproducibility, but minor edge cases may still exist.

---

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ 00_data_engineer
â”œâ”€â”€ 01_data_warehouse
â”œâ”€â”€ 02_data_analyst
â”œâ”€â”€ 03_data_scientist_01
â”œâ”€â”€ 04_data_scientist_02
â”œâ”€â”€ db_init
â”œâ”€â”€ src
â”œâ”€â”€ scripts
â”œâ”€â”€ etl
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â””â”€â”€ requirements.txt
Each module represents a distinct stage of the data lifecycle.

ğŸ§© Modules Overview
00 â€” Data Engineer
Database foundations and infrastructure

PostgreSQL initialization

Schema creation

Data availability verification

Reproducible environments (Docker-based)

Focus: reliable data foundations and infrastructure setup.

01 â€” Data Warehouse
SQL-based data preparation

Monthly data ingestion

Dataset consolidation

Deduplication strategies

Data fusion into analysis-ready tables

Focus: transforming raw data into a single, consistent source of truth.

02 â€” Data Analyst
Exploratory analysis and visualization

Distribution analysis

Boxplots and outlier detection

Histograms and frequency analysis

Clustering preparation and elbow method

Focus: understanding data behavior and extracting insights through visualization.

03 â€” Data Scientist I
Feature analysis and preprocessing

Feature distributions

Class comparisons (Jedi vs Sith)

Normalization and standardization

Train/test dataset preparation

Focus: preparing clean and meaningful features for modeling.

04 â€” Data Scientist II
Modeling and evaluation

Confusion matrices

Correlation heatmaps

Feature selection

Decision trees

KNN and ensemble voting

Focus: model behavior, interpretability, and evaluation logic.

## ğŸ“Š Visual Insights

Below are selected visual outputs that summarize key analytical insights from the project.

### Purchase Price Distribution

This boxplot shows the overall distribution of purchase prices, highlighting a strong right skew
and the presence of significant outliers.

![Overall Purchase Price Distribution](mustache_overall.png)

---

### User Behavior Analysis

These histograms analyze user purchasing behavior for users whose total spending is below **225 A$**:

- **Left:** purchase frequency per user  
- **Right:** total spending per user  

They reveal that most users make few purchases and spend relatively small amounts,
with a clear long-tail distribution.

![User Behavior Histograms](building_histograms.png)




ğŸ§  Design Decisions
Datasets, subjects, and evaluation PDFs are excluded
This repository focuses on workflows and logic rather than academic artifacts.

Generated outputs are not versioned systematically
Plots and predictions can be regenerated by executing the scripts.

Clear separation of concerns
Each module maps to a real data role and a specific responsibility.

ğŸ” Reproducibility & Execution
This project does not include proprietary datasets.

SQL scripts can be executed against a PostgreSQL instance

Python scripts expect CSV inputs with documented or inferable schemas

Visualizations and predictions are reproducible by running the scripts

Python Environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
Docker (optional)
docker compose up --build
ğŸ¯ Purpose
This repository serves as:

a technical portfolio of data foundations

a reference for structured data workflows

a demonstration of progression from data engineering to data science

It reflects a production-aware and role-oriented approach to working with data.

ğŸ“š Background
This project was originally developed in an academic context and later refactored and curated
to meet professional and public portfolio standards.











